
**Entry 5**

**Kallisto pipeline - 1st run**
This is the code I used to run Kallisto. I wanted to begin with a test of running the Kallisto -> Sleuth pipeline locally on my personal computer. If these algorithms are as fast and memory effective as they claim, it should be fine to run locally. It will keep an eye at the time it takes for the different parts of the pipeline.

The authors of Sleuth recommends 100 bootstraps of Kallisto prior to using Sleuth for differential expression. Keep in mind that I i'm working with approx 25M reads/sample, 2x125bp PE. Quite a lot in other words, it will be interesting to see how fast we can get this running locally with 100 bootstrap.

**(1). Build Kallisto transcriptome index**

    kallisto index -index=HumanGTCh38cDNAncRNA_Kallisto_INDEX Homo_sapiens.GRCh38.cdna.all.fa Homo_sapiens.GRCh38.ncRNA.fa

I clocked the time it took to create this index, and it clocked at approx: 5 minutes. Note that this was performed running locally on my personal laptop. The target de Bruijn graph had 1275418 contigs and contained 123570192 k-mers

**(2). Run the Kallisto quantification algorithm**

    kallisto quant --index= HumanGTCh38cDNAncRNA_Kallisto_INDEX -b 100 FR1.fa.gz  FR2.fa.gz  FR3.fa.gz  FR4.fa.gz  FR5.fa.gz RR1.fa.gz  RR2.fa.gz  RR3.fa.gz  RR4.fa.gz  RR5.fa.gz

(paired end reads, 5 lanes).
Clock: about 40 minutes.

From prior experience of trying to run mapping-pipelines locally, I'm quite impressed by the speed. I find this interesting to note here, as if these pipelines perform equal well, the computational cost could mean a lot. For us that performs this project, we do not lack computational resources and hence, are not really affected by computational time, but one could easily imagine settings where the computational load is of uttermost significance and a bottleneck in a pipeline. Overall reduction is always desirable.

**Output**
Of of the 32mil processed reads, I got 8mil reads psuedoaligned. My first reaction of this number would be that it seems to low? Lets see what STAR mapping gives, but I would expect STAR to map approx >80% of the reads. However, the sample I tested on was virus infected, and I don't have viral transcripts included, unsure what amount of total reads they represent, I don't think it should be that high...

![Psuedoalignment]({{ site.url }}/assets/images/kallisto_first_run.png)

Another thing that seems weird is the "estimated average fragment length" of 154? The sequences are 125bp. I got unsure here, so I ran a fastQC on one of the read fasta files to double check. And yes, the sequences are 125 average length:

![Psuedoalignment]({{ site.url }}/assets/images/fastQC_check.png)

Why do the Kallisto algo give me this estimated average length?
In the manual it says *" For paired-end reads, the average fragment length can be directly estimated from the reads and the program will do so if -l is not used (this is the preferred run mode)."*
Maybe I should try to state the average length of the reads as an input parameter. However, you shouldn't have to do that...

Next step would be to run one more sample (non-infected) to be able to do a first differential expression analysis via Sleuth.
