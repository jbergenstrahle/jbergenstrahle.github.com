---
layout: page
title: "Entry9  - 25/12-16"
description: "Entry8"
---
{% include JB/setup %}

**Continuation -- looking at the difference in output**

In the endeavour for a deeper understanding of the correlation in my output data I took both data sets (i.e. from kallisto and STAR/HTSeq-count pipelines), and extracted the geneIDs that were present in both samples. As already noted, this mean that I filtered away quite many genes from the kallisto data set, while the STAR/HTSeq-count data kept most of the genes. After extracting, I did a correlation table between the data sets.  


             D10_Cond_1 D10_Cond_6 D11_Cond_1 D11_Cond_6
    sample1  0.9016576  0.7985663  0.8238340  0.7574746
    sample2  0.7985510  0.9219731  0.7799343  0.8874375
    sample3  0.8156985  0.7720145  0.9193470  0.7643299
    sample4  0.7676744  0.9029451  0.7832623  0.9094485

This is a person correlation between the estimated counts from the Kallisto pipeline, against the raw count values from the STAR pipeline. Sample against sample shown as the diagonal line, i.e. >90%. 
Computing the same type of test for the log2foldchanges after DESeq2 will result in very low overall correlation. Which of course is no surprise since there are a lot of low count noise in the data,
and noisy genes with log counts will have an un-proportional influence on the correlation (even though the LFC shrinkage of DESeq2 will make it a little bit less). 
Instead I thought that it could be meaningful to look at the log2foldchange correlation **in the op DE genes**, since these genes are the interesting part we get out of our experiment, and actually
base our biological hypothesis upon. So, I ordered the genes according to FDR level and extracted the top 100 from each pipeline. Pearson correlation gave me the following output:

    Correlation: 0.984
    
Which to me sounds like a quite OK correlation among the top DE genes.

I created an interactive plot for the top 89 most DE genes in order to easily detect outliers and which geneIDs they belong to:
http://rpubs.com/jbergenstrahle/235948

The geneID that immediately catch your eye is "SLC38A5", which show a substantial difference between the two workflows. The log2foldchange from STAR/HTSeq-count (red) is **6.38**, while the Kallisto approach gives a value of **4.57**. One might speculate that there are some characteristics of this transcript that makes it more probable for detection in the STAR/HTSeq-count scheme, but, we are talking differential expression results here, and such a characteristic should be present across samples, i.e. the effect of transcript identification differences should be "normalized" away. This should be especially true for transcripts of high abundance where noise do not constitute a significant factor. Accordingly I went on by looking at the "raw"-counts values that I use as input into DESeq2:

 STAR/HTseq-count:

                    sample1 sample2 sample3 sample4
     ENSG00000017483      6     611       4     895

           
Kallisto:

			        sample1 sample2 sample3 sample4
    ENSG00000017483                   7     617       7     914


This strikingly not a lot of difference here. Two very different counting techniques end up with very similar count values. However, the final output from DESeq2 in terms of log2foldchange would trick one into thinking that the approaches differ substantially.  

In order to get a better feeling of the impact that the raw-count values have on the end result, I then made a manual edit of the output file from STAR/HTSeq-count and changed the count values to match the kallisto count values. After re-running the pipeline, I ended up with a log2foldchange of **6.08**. As we clearly see, changing the raw count values does not have such a huge impact on the LFC value in the end. 

I then went on, wondering if the LFC shrinkage that DESeq2 performs have any different impact on the final output. The LFC shrinkage effect can be showed via the dispplot function in DESeq2:

![plotdisp]({{ site.url }}/assets/images/plotDispfunction.png)

The black dots are the gene data that we have used as input. From this a gene-wise maximum likelihood estimate (MLE) is obtained. Then, a curve (red) is fit to the MLE to capture the overall trend of dispersion-mean dependence. As we expect and can see from the figure, we have a high dispersion for low counts.
The fitted curved is then used as input for a second estimation round, which results in the final maximum a posteriori (MAP) estimates of dispersion. The final estimates are shown in blue, i.e. the black dots are "shrunken" towards the fitted red line and the result is the blue dots. The dots that have a circle around them are detected as outliers and will be put aside during the fitting of the model. The shrunken LFCs together with their standard errors are used in a "Wald test" for differential expression.
In short, the shrinkage algorithm uses the largest fold changes that are not due to low counts and uses these as the prior distribution (these have biggest effect).

The effect of the shrinkage can be visualized via removal of the LFC shrinkage during DESeq2 pipeline, here I tried that and plotted:

![LFCshrink]({{ site.url }}/assets/images/LFC_shrink.png)

In the plot I marked the SLC38A5 gene to be able to follow the effect of LFC shrinkage for this gene. As we see, the effect is not that large for this gene in particular. Which is expected, since the gene did show quite 
consistent count-values across the conditions. **If** we would have e.g. one of the virus-infected samples show a very high count-value and the other virus-infected sample a low count value, the LFC shrinkage would be much more pronounced. This is not the case here. And even if that **was** the case, the difference we see between the methods would not be derived from the LFC shrinkage per se. Instead I ended up finding the answer to the question in the function that I use to import the Kallisto counts to the DESeq2 pipeline, which is made via the "tximport" package. The method that is used utilize the transcript-level **abundance estimates** in order to perform a calculation of an offset that corrects for changes to the average transcript length across the samples. The creators of this package makes the argument (https://f1000research.com/articles/4-1521/v2) that accounting for the varying average transcript lengths across samples leads to substantially improved false discovery rates during DE analysis compared to just simple aggregation of counts directly. The improvement is derived from how this will effect the genes with changes in isoform composition. The authors used stimulated data sets where they deliberately introduced strong such effects, and they note that the effect on real data sets will depend on the extent that differential transcript abundances is present, and that in most real data sets the effect of this will probably not be that large. I think that I just stumbled upon one such example where this effect clearly gives a significant difference. The SLC38A5 have quite many splice variants compared to the other DE genes present (14), and individual DTU with different transcripts-lengths could have an substantial impact on the DE result. The number of different transcripts is however not of importance here, but the **length** of the different transcripts are. For this particular gene, the longest transcript variant is 2659bp, while the shortest protein coding transcript is 622bp. 

The following plot is based on the output from Kallisto, and shows the estimated counts for the different isoforms of the gene SLC38A5. I have removed all isoforms that had 0 counts across all samples. The blue dots are from the "medium" condition, and the red dots from the "virus" condition. The size of the dots corresponds to the length of the isoform. The bigger the dot, the longer the isoform. As we can note here, the virus treated samples have overall more counts (in accordance with our DE analysis), but specifically, they have more counts of the longer isoforms. A long transcript will in theory lead to the observation of more reads than a short transcript. Hence, when using the STAR/HTSeq-count pipeline, in the union mode as I did, we will get a higher log2foldchange compared to the kallisto pipeline, due to the fact that we don't take into account the skewness of the isoform distribution between the samples. The kallisto pipeline, will take into account the length of the transcripts and the longer transcript will be "less weighted" in comparison to the shorter ones, as they in theory should produce more reads. 

![estcounts]({{ site.url }}/assets/images/est_counts.PNG)

We are working with very few samples here (2 of each condition), and the count values for SLC38A5 are not that high. Hence, the DE effect that we see should be regarded with care. One can somewhat confidently say that SLC38A5 is higher expressed in the virus-infected genes, but the degree of expression should be considered with the low power in mind. However, it is interesting to see an clear example of where and when then two pipelines can differ quite substantially, even though they end up in the same conclusion in this case, namely - that SLC38A5 is up-regulated during viral infection of moDCs. 

It should be noted however, that among the most differential expressed genes, this gene is an outlier and the overall "story" of DE genes does not seem to differ between the pipelines.

Interestingly to note, is that the mapping based method seems to consistently report higher log2foldchanges across the genes found to represent the most differentially expressed genes.
In order to look if my theory regarding the differential transcript usage holds for the other genes as well, I also looked at the following two genes: FRMD3, MX2, RSAD2 (higher LFC in STAR/HTSeq-count) and EIF2AK2 (lower LFC in STAR/HTSeq-count).
The plotting approach may be a bit hard to interpret, so I turned to the more simple approach of calculating the count-based average transcript length of the samples (Medium vs Virus) from the Kallisto pipeline.
They all showed consistency in regard to the fact that for the samples that display a smaller LFC change **for the unregulated genes in response to virus infection **, all have a higher mean transcript length in the virus samples based on the counts from Kallisto.
And visa versa, the sample with higher LFC change in the Kallisto pipeline, had a shorter transcript length in the virus samples. This makes sense, as the longer transcripts will **per count** be given less influence as the shorter transcripts.
Accordingly, the STAR/HTseq-count will overestimate or underestimate the LFC since the length differences of individual transcripts are not taken into account. Note here that I say over/underestimate here like the Kallisto output is the "correct" answer,
this is of course a claim I cannot make, since we do not have the true abundances of the transcripts in this real data sets, and there is now way for us to actually test the abundances via other measures at this time.

Notice: I re-did the exact same with the comparison Medium vs Polyl, which display very similar output as the comparison shown above.
Biologically wise, it's also of interest to note how similar the the virus and polyl treatments affect the samples. Apart from some differences, more or less the same genes are upregulated, with some variance in the degree of upregulation.

All of the most upregluated genes are present in both comparisons. Not really that surprising given that polyl:C is used as a immune-response stimulant, and that virus infection per se induce such a response.
This can also act as an sanity check for the pipelines, as they give the same result regarding the biological interpretation. In the last entry I stated that I would create a list of genes known to react in response to infection.
I created this list, and made MA-plots with the specific genes labeled. The result is highly consistent both across comparisons and across non-alignment vs alignment based methods:


![MA-plots-labeled]({{ site.url }}/assets/images/MA-plots-labeled.png)

"ISGs" = Interferon stimulated genes. Notice the high similarity between the pipelines one can decipher just by a quick visual inspection. 


**Sanity checking**

After I had done the above analysis, I got the idea that one should check the p-value distribution of the DESeq2 output as a sanity check.
Specifically regarding the fact that there are some discussion among the bioinformatic community regarding how appropriate it is to use tools as DESeq2, which are based upon the underlying assumption of negative binomial distributions, on estimated count data like the output from Kallisto.
Some argue that this is not appropriate due to the estimated counts not following this distribution. While others argue that this doesn't really impose any problem and performing the workflow as I did above is fine.
I thought that p-value distributions and dispersion mean relationships might give some clue whether the data looks "healthy" or not.

![pva-histo1]({{ site.url }}/assets/images/pval_hist_kallisto_vs_STAR.png)

We do see a large peak at very low p-values, which is as expected given the different conditions (virus infected cells should really have a different transcriptomic profile than non-infected).
However, there is some tendency for the p-values at the right side of the peak in the Kallisto output, which in theory should be more or less "flat", i.e. the probability of obtaining a p-value should be uniform.
However, this seems only to be the case for the virus comparison, which does not really make much sense. My guess would be that this is noise, which happened to manifest itself in a way which could rise your eyebrow.
Without any real logic behind the test, I just wanted to re-run Kallisto using only the cDNA reference as transcriptome (previous round was with the extra addition on ncRNA). Looking at the p-values here gave me:

![pval-histo2]({{ site.url }}/assets/images/pval_hist_kallisto_with_and_without_ncRNAref.png)

The tendency of weird looking distribution regarding the virus comparisons is not evident here. I would say that the kallisto output in general do look healthy regarding the p-value distributions. With the caveat of the first plot in mind.
Moving forward I also looked at the dispersion mean variances:

![dispPlot-comparison]({{ site.url }}/assets/images/dipsersion_variance_plots.png)

My take is that from these observations at least, there does not really seem to be a problem with aggregation of estimated transcript-counts, rounding to produce integers, and using them as input to negative binomial-distribution based DE tools. 

**Remaining Parts of the course project**

Other team-members are responsible for performing the same comparisons with the use of another alignment-independent tool (Salmon). I will await their output to see how this correlate to the pipelines I have used here. 
